了解しました。拡散モデルのSDE（確率微分方程式）形式に関する理論的背景とPythonによる実装例を、大学院レベルの数学的視点と直観的な解説の両方を交えてまとめます。
少々お時間をいただきますが、完了次第ご報告します。

# 拡散モデルのSDE形式：理論的背景とPython実装

## はじめに
拡散モデル（Diffusion Models）は**データに徐々にランダムノイズを添加し、同時にその拡散過程を逆転させてデータを生成する**という発想に基づく生成モデルです ([What are Diffusion Models? | Lil'Log](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#:~:text=Diffusion%20models%20are%20inspired%20by,same%20as%20the%20original%20data))。この手法は非平衡熱力学に着想を得ており、拡散（ディフュージョン）過程によるエントロピー増大と、その逆過程としての**ノイズ除去（デノイジング）**を利用して新しいデータサンプルを得ます。特に近年は、スコアベースモデルやDenoising Diffusion Probabilistic Model (DDPM)など、拡散モデルがGANに匹敵する高品質なサンプルを生成できることが示され注目を集めています ([What are Diffusion Models? | Lil'Log](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#:~:text=Diffusion%20models%20are%20inspired%20by,same%20as%20the%20original%20data))。本稿では、**拡散モデルの確率微分方程式（SDE）による定式化**について、大学院レベルの理論的背景（Ito過程、Fokker–Planck方程式、逆拡散過程、確率流など）から、実装（Euler–Maruyama法による数値解法）までをまとめます。併せて、Score-based Generative Models ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=,SDE%20solvers%20to%20generate%20samples))におけるSDE定式化と確率流ODE（Probability Flow ODE）との関係についても解説し、なぜSDEでモデル化するのかといった物理的・直観的理解にも触れます。最後に、Pythonによる簡単な実装例を示し、理論と実装のつながりを確認します。

## 確率微分方程式とフォッカー–プランク方程式の基礎
まず、拡散モデルの連続時間版を理解するために**確率微分方程式（SDE）**と**フォッカー–プランク方程式**の基礎を押さえます。一般に、$\mathbb{R}^d$上の**伊藤過程（Itô過程）**は次の形で表されます。

- **Ito過程の一般形**： $d\mathbf{x}_t = f(\mathbf{x}_t, t)\,dt + g(\mathbf{x}_t, t)\,dW_t$ 

ここで$\mathbf{x}_t$は時刻$t$における確率過程の状態、$f(\mathbf{x},t)$は**ドリフト項**（決定論的な移動成分）、$g(\mathbf{x},t)$は**拡散係数**（ノイズの強さ）、$W_t$は標準**ウィーナー過程**（標準的なブラウン運動）です。直感的には、無限に小さい時間間隔$dt$の間に、状態$\mathbf{x}$が平均$ f(\mathbf{x},t)\,dt$だけ変化し、分散$g(\mathbf{x},t)^2\,dt$のガウスノイズが加わると考えるとよいでしょう。解$\mathbf{x}_t$は確率的な過程（**拡散過程**）であり、解析的に解くことは難しい場合が多いですが、確率論および数値計算の枠組みで取り扱われます。

**Fokker–Planck方程式（FP方程式）**は、このSDEで記述される確率過程の**確率密度関数の時間発展**を支配する偏微分方程式です（**前向きコルモゴロフ方程式**とも呼ばれます）。状態$\mathbf{x}$の確率密度を$p_t(\mathbf{x})$とすると、FP方程式は以下の形になります ([FokkerPlanck_Equation](http://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/#:~:text=In%20fact%2C%20we%20can%20also,nabla%5Ccdot%20%5Bfp_t%2B%5Cfrac%7B1%7D%7B2%7D%5Cnabla%5ET%5Bgg%5ETp_t%5D%5D%5C%5C%20%5Cend%7Balign%2A%7D))。

- **Fokker–Planck方程式**（連続の式の形）： 
$$ 
\frac{\partial}{\partial t}p_t(\mathbf{x}) = -\nabla_{\mathbf{x}}\cdot\Big( f(\mathbf{x},t)\,p_t(\mathbf{x}) \Big) \;+\; \frac{1}{2}\nabla_{\mathbf{x}}^2:\Big( g(\mathbf{x},t)g(\mathbf{x},t)^T\,p_t(\mathbf{x}) \Big)\,. 
$$

右辺第1項はドリフト項による**確率流**の発散を表し、第2項は拡散ノイズによる分布の広がりを表す拡散項です ([FokkerPlanck_Equation](http://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/#:~:text=In%20fact%2C%20we%20can%20also,nabla%5Ccdot%20%5Bfp_t%2B%5Cfrac%7B1%7D%7B2%7D%5Cnabla%5ET%5Bgg%5ETp_t%5D%5D%5C%5C%20%5Cend%7Balign%2A%7D))（$\nabla^2:\!(\cdot)$は拡散項の2階微分を示唆する記法です）。特に$g$が空間に対して一様（$\mathbf{x}$に依存しない）な場合には、拡散項は$\frac{1}{2}g(t)^2\nabla_{\mathbf{x}}^2 p_t(\mathbf{x})$と簡略化されます。FP方程式は、確率密度の質量保存則に対応する**連続の方程式**でもあり、ドリフト項と拡散項による**確率密度の流れ（確率流）**の収支を記述しています ([FokkerPlanck_Equation](http://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/#:~:text=An%20equation%20in%20such%20a,have%20a%20very%20intuitive%20explanation)) ([FokkerPlanck_Equation](http://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/#:~:text=2,Therefore%2C%20we))。このように、SDE（微視的な1サンプルの運動方程式）とFP方程式（巨視的な確率密度の方程式）は対をなしており、片方が与えられればもう片方を導出できます。

## 拡散モデルにおけるSDE定式化 (Score-based Generative Models)
拡散モデルは本来、**有限ステップのマルコフ連鎖**（有限時間差分でのノイズ付加と除去）として定義されました ([What are Diffusion Models? | Lil'Log](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#:~:text=Diffusion%20models%20are%20inspired%20by,same%20as%20the%20original%20data))。しかし、Yang Songら(2021)はこの枠組みを**連続時間の確率微分方程式**に拡張し、従来の手法（スコアマッチングによる生成モデルや拡散確率モデル）を包含する一般的な定式化を提案しました ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=We%20show%20that%20this%20framework,In%20addition%2C%20we))。その概略は以下の通りです。

1. **順拡散（Forward SDE）**: 時刻$t=0$で**データ分布**$\displaystyle p_0(\mathbf{x})$からサンプルを取り出し、ある決められたSDEに従って**徐々にノイズを注入しながら**時刻$t=T$まで状態を発展させます。この過程によって、複雑なデータ分布$p_0$が徐々に平滑化され、最終的には**既知の単純な分布**$p_T(\mathbf{x})$（例えば多次元ガウス分布）へと近づきます ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=,SDE%20solvers%20to%20generate%20samples))。言い換えると、「**データからノイズを作り出す**」過程です ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=,SDE%20solvers%20to%20generate%20samples))。この順方向過程を記述するSDEはモデルの一部として設計するもので、例えば画像生成では以下のようなSDEが経験的に有効だと知られています ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=perturbs%20data%20with%20a%20Gaussian,VP%20SDE)):
   - **VP-SDE（Variance Preserving SDE）** ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=perturbs%20data%20with%20a%20Gaussian,VP%20SDE)): 離散時間のDDPMに対応するもので、例えば $d\mathbf{x} = -\frac{1}{2}\beta(t)\mathbf{x}\,dt + \sqrt{\beta(t)}\,dW_t$ といった形のSDEです。$\beta(t)$は時間とともに変化するノイズ強度（variance schedule）で、ドリフト項 $-\frac{1}{2}\beta(t)\mathbf{x}$ が付加されたノイズに合わせて状態を減衰させ、分散を一定範囲に保ちます。 
   - **VE-SDE（Variance Exploding SDE）** ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=perturbs%20data%20with%20a%20Gaussian,VP%20SDE)): 初期には微小なノイズしか加えず、時間とともに**分散が爆発的に増加**するようなSDEです（例: $d\mathbf{x} = 0\,dt + \sigma(t)\,dW_t$ で$\sigma(t)$が増加関数）。ドリフトが無い代わりにノイズ項が強くなり、分布の分散が単調に大きくなっていきます。
   - **その他**: 上記以外にもSub-VP SDE ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=provide%20three%20SDEs%20that%20generally,VP%20SDE))などが提案されていますが、基本的にはデータに徐々にノイズを混ぜていき、最終的に高分散のノイズ優勢な状態にする設計になっています。

   いずれのSDEを選ぶにせよ、$t=T$では**事前分布**（ノイズの基底分布）$p_T(\mathbf{x})$として多変量正規分布（例えば$\mathcal{N}(\mathbf{0}, \sigma_T^2 \mathbf{I})$）等が得られるようパラメータ$\beta(t)$や$\sigma(t)$が調整されます。言い換えると、$t=0$のデータが**物理的な拡散過程**によってゆっくりと「熱浴」に浸され、$t=T$では乱雑なノイズ状態になっているということです。

2. **逆拡散（Reverse SDE）**: 上記の順拡散過程を**時間逆にたどる**ことで、ノイズからデータを生成します ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=,SDE%20solvers%20to%20generate%20samples))。しかし、通常の物理では拡散の逆過程は実現できません。そこで登場するのが**スコア関数**です。確率微分方程式には**対応する逆向きのSDEが常に存在**し、Anderson (1982) によれば**そのドリフト項には元の確率密度の勾配が現れる**ことが知られています ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=%5C%5B%5Cbegin,equation))。Songのフレームワークでは、$p_t(\mathbf{x})$を順拡散で得られる中間分布とすると、その**時間逆SDE**は次式で与えられます ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=%5C%5B%5Cbegin,equation))。

   \[
   d\mathbf{x} = \big[\,f(\mathbf{x}, t) \;-\; g(t)^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\,\big]\,dt \;+\; g(t)\,d\bar{\mathbf{w}}\,,
   \] 

   ここで$d\bar{\mathbf{w}}$は**逆時間のウィーナー過程**（時間を逆転したノイズ項）です ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=Here%20%5C%28%5Cmathrm,x))。直感的には、順方向で加えた微小ノイズ$g(t)dW_t$に対し、逆方向では**同じ強度のノイズを再び加えつつ**、追加のドリフト項$-g(t)^2 \nabla_{\mathbf{x}}\log p_t(\mathbf{x})$が**分布の山（高密度領域）に向かう力**として作用します ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=%5C%5B%5Cbegin,equation))。この追加項により、ただノイズを引き算するだけでは成し得ない「秩序の復元」（すなわちデータらしい構造の形成）が可能になります。$\nabla_{\mathbf{x}}\log p_t(\mathbf{x})$は$p_t$の対数微分であり**スコア関数**と呼ばれます。つまり**スコア関数が分かれば逆拡散過程を構築できる**わけです ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=Here%20%5C%28%5Cmathrm,x))。

   > **補足（物理的直観）**: 順拡散はインクが水に拡散していくように情報が広がり無秩序になる過程（エントロピー増大）ですが、逆拡散はそれとは逆に、水中に拡散したインクが再び集まって元の形を成すような過程です。物理法則の下ではエントロピー減少は起こりませんが、スコア関数は「どの方向に密度が高いか」という指針（いわば**負のエントロピー勾配**）を各状態に与えます。したがって逆拡散SDEでは、この指針に沿って粒子（サンプル）を移動させる微視的な力が働き、同時にランダムな攪拌も加えることで、全体としては目標分布へ緩やかに収束するランダム過程が実現されます。これはノイズを一度に除去せず**徐々に（ランダム性を残しつつ）除去**することで、高次元空間でもモード崩壊せず安定的に分布を生成できる鍵となっています。

3. **スコア関数の推定（学習）**: 上記の逆SDEを実現するには、本来であれば各時刻$t$における真の分布$p_t(\mathbf{x})$のスコア$\nabla \log p_t(\mathbf{x})$が必要です。しかし$p_t(\mathbf{x})$はデータ分布から順拡散で得たものなので解析的には不明です。そこで、**時間を条件とするスコア推定モデル**$s_\theta(\mathbf{x}, t)$（ニューラルネットワークで実装）を訓練し、$s_\theta(\mathbf{x}, t) \approx \nabla_{\mathbf{x}}\log p_t(\mathbf{x})$を学習します ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=Estimating%20the%20reverse%20SDE%20with,based%20models%20and%20score%20matching))。この学習には**スコアマッチング**と呼ばれる手法が用いられ、例えば**拡張された形式の誤差関数** $ \mathbb{E}_{t,\mathbf{x}(t)}[\lambda(t) \|s_\theta(\mathbf{x}(t),t) - \nabla_{\mathbf{x}(t)} \log p_t(\mathbf{x}(t))\|^2]$ を最小化することで推定が行われます（$\lambda(t)$は時間による重み付け関数） ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=minimizing%20KL%20divergences%20and%20maximizing,art%20autoregressive%20models))。この訓練は、データサンプルに様々な強さのガウスノイズを加えたもの$\mathbf{x}(t)$に対し、そのノイズを除去する方向ベクトル（真のスコア）を当てられるようにモデルを調整するイメージです。訓練が十分にできれば、逆SDEにおいて**$-g(t)^2 s_\theta(\mathbf{x},t)$がノイズ除去の適切な方向**を示すドリフトとして作用し、$d\bar{\mathbf{w}}$によるランダム摂動と相まってデータ分布へのサンプル生成が可能となります ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=Estimating%20the%20reverse%20SDE%20with,based%20models%20and%20score%20matching))。

以上が拡散モデルのSDE定式化における一連の流れです。この連続時間の枠組みは、**従来の離散時間拡散モデルやスコアベースモデルを包含**することが論文中で示されています ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=We%20show%20that%20this%20framework,In%20addition%2C%20we))。実際、拡散モデルにはマルコフ連鎖（有限ステップ）、確率微分方程式（連続時間）、確率流ODE（後述）など**形式上は異なる定式化がいくつか存在しますが、それらは同等の分布変換を実現する**ことが知られています ([Diffusion model - Wikipedia](https://en.wikipedia.org/wiki/Diffusion_model#:~:text=There%20are%20various%20equivalent%20formalisms%2C,typically%20%20171%20or%20transformers))。

## 確率流ODEとSDEの関係
上記の逆SDEは乱数に依存する**確率的な生成過程**ですが、これと**同じ一連の中間分布$\{p_t\}$を再現する****常微分方程式（ODE）**の存在も示されています ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=In%20%2C%20we%20show%20t,flow%20ODE%20%2C%20given%20by))。これが**確率流（Probability Flow）ODE**と呼ばれるものです。確率流ODEは、対応するSDEから**拡散項（ノイズ項）を取り去り、その効果をドリフト項に織り込んだ**決定論的なシステムとみなせます ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=probability%20flow%20ODE%20%2C%20given,by))。具体的には、SDEのFokker–Planck方程式において**確率流がゼロ**になるような**流れ場**を構成することで得られます ([FokkerPlanck_Equation](http://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/#:~:text=p_t%28x%29%5Cright%5Dp_t%28x%29%5Cquad%5Ctext%7B%28derivative%20of%20log%29%7D%5C%5C%20%3D%26%20,ODE%29%7D%20%5Cend%7Balign%2A%7D))。式で表すと、先ほどのSDEに対応する確率流ODEは次のようになります ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=probability%20flow%20ODE%20%2C%20given,by))。

- **確率流ODE**: 
\[ d\mathbf{x} = \Big[\,f(\mathbf{x},t) \;-\; \frac{1}{2} g(t)^2\,\nabla_{\mathbf{x}}\log p_t(\mathbf{x})\Big]dt\,. \]

SDEの逆過程と比べると、ノイズ項が無くなり、その代わりに**ドリフト項のスコアに伴う係数が$-\frac{1}{2}g(t)^2$に変わっている**ことがわかります ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=probability%20flow%20ODE%20%2C%20given,by))（上述の逆SDEドリフトは$-g(t)^2 \nabla \log p_t$でした）。このODEを時間$t=0$から$T$まで解くことで、データ分布$p_0$を**全くランダム性を伴わず**に徐々にノイズ分布$p_T$へ写像できます。また逆向き（$T$から$0$へ）にこのODEを解けば、ノイズからデータを決定論的に再構成できます ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=Image%20%20We%20can%20map,obtained%20by%20estimating%20score%20functions))。重要なのは、この確率流ODEによる生成過程も、元のSDEと**同じ中間分布系列**$\{p_t\}$を持つことです ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=The%20following%20figure%20depicts%20trajectories,distributions%20as%20the%20SDE%20trajectories))。図示すれば、確率流ODEに従う軌道は対応するSDEの軌道よりも滑らかですが、時間毎の分布は両者で一致します ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=The%20following%20figure%20depicts%20trajectories,distributions%20as%20the%20SDE%20trajectories))。

確率流ODEの利点の一つは、**生成過程を完全に決定論的にできる**ため**確率的な揺らぎによるサンプル品質のばらつきが無い**こと、そして**確率流ODEを通じてモデルの（近似）尤度計算が可能になる**ことです ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=When%20%5C%28%5Cnabla_%5Cmathbf,and%20is%20fully%20invertible)) ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=As%20such%2C%20the%20probability%20flow,with%20numerical%20ODE%20solvers))。後者について補足すると、確率流ODEは連続的な正規izingフロー（continuous normalizing flow）とみなせるため、インスタント**変数変換の公式**（instantaneous change-of-variable formula）によって$p_T$（例えば標準正規）の既知の密度から$p_0$（データ）の密度を積分的に計算できます ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=When%20%5C%28%5Cnabla_%5Cmathbf,and%20is%20fully%20invertible)) ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=As%20such%2C%20the%20probability%20flow,with%20numerical%20ODE%20solvers))。これは従来のスコアマッチングベースのモデルでは困難だった**厳密な対数尤度の評価**を可能にする点で、大きな理論的利点です。もっとも、確率流ODEでサンプル生成を行う場合、SDEに比べてランダム性が無い分モデルの多様性に制限が出る可能性があります。そのため、実用上はSDE経由の生成にMCMC的補正を加えるPredictor-Corrector法 ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=As%20a%20consequence%20of%20these,dynamics%20and%20Hamiltonian%20Monte%20Carlo))や、ODE/SDEを高速に解く工夫（高次の数値解法やステップ幅の最適化）が研究されています ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=directly%20employed%20to%20solve%20the,samples%20faster%20with%20better%20quality)) ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=to%20Euler,samples%20faster%20with%20better%20quality))。いずれにせよ、確率流ODEは拡散モデルの理論を豊かにし、**生成モデルを確率過程（SDE）としても決定論的変換（ODE）としても捉えられる**ことを示した重要な成果です。

## Euler–Maruyama法による逆拡散SDEの実装
最後に、学習済みのスコアモデル$s_\theta(\mathbf{x},t)$を用いて**逆拡散SDEからサンプルを生成する**方法を、数値解法とコード例の両面から説明します。確率微分方程式の数値解法には様々な手法がありますが、基本的かつ代表的なものが**Euler–Maruyama法**です ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=By%20solving%20the%20estimated%20reverse,approx%200)) ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=Here%20%5C%28%5Cmathbf,functions%20perturbed%20with%20Gaussian%20noise))。これは常微分方程式のオイラー法を確率過程に拡張したもので、時間を微小ステップに離散化し、その間のノイズ項を正規乱数で近似します。先述の逆拡散SDE（式\eqref{rsde}に対応）の場合、Euler–Maruyama法による離散時間更新則は次式となります ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=following%20procedure%20until%20,0))。

\[ 
\mathbf{x}_{t+\Delta t} \;=\; \mathbf{x}_t \;+\; \Big[\,f(\mathbf{x}_t, t) - g(t)^2\,s_\theta(\mathbf{x}_t, t)\Big]\,\Delta t \;+\; g(t)\sqrt{|\Delta t|}\,\mathbf{z}_t\,,
\] 

ここで$\mathbf{z}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$は各ステップで新たにサンプリングする標準正規乱数ベクトルです ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=%5C%5B%5Cbegin,aligned))。注意すべきは、この式では$\Delta t$は**負の小時間**（すなわち時間を遡る方向）を表すため、実装上は$\Delta t$の符号に注意して更新を行います ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=steps%20and%20small%20Gaussian%20noise,approx%200))。例えば時間を$N$ステップに離散化しステップ幅を$h = T/N$とすると、$t_n = T - n h$（$n=0,\dots,N$）で刻み、次のように更新します。

- **逆拡散SDEの離散更新（擬似コード）**:
  ```python
  x = 標準正規乱数サンプル  # 初期値: t=Tでのノイズサンプル
  for n in range(N, 0, -1):    # t=T から t=0 へ逆方向にループ
      t = n * h                # 現在の時刻
      z = 標準正規乱数(サイズ=xと同じ) 
      # ドリフト項とスコア項を計算
      drift = f(x, t)                  # 順SDEのドリフト f(x,t)
      score = s_theta(x, t)            # 学習済みスコアモデルの出力
      # Euler–Maruyamaによる更新
      x = x - (drift - g(t)**2 * score) * h + g(t) * sqrt(h) * z
  # ループ終了時、x が生成されたサンプル（t=0付近） 
  ``` 

  上記のコードでは、$f(x,t)$および$g(t)$は選択したSDEに対応する関数（例えばVP-SDEの$f=-\frac{1}{2}\beta(t)x$, $g=\sqrt{\beta(t)}$）で、$s_\theta(x,t)$は学習済みのスコア関数近似モデルです。更新式で`x = x - (drift - g(t)**2 * score) * h + ...`と**マイナス符号**が入っているのは、$\Delta t = -h$（負方向のステップ）を代入したことに対応しています。実際、前述の数式を$\Delta t < 0$で適用する代わりに、$\Delta t = h>0$として式全体にマイナス符号を付けることで実装上の整合性が取れます。このようにして時間を少しずつ巻き戻しながら状態$x$を更新していくと、$t=0$付近では初期データ分布$p_0(\mathbf{x})$に従うサンプル$x$が得られます。

 ([image]())以下に、単純な1次元の例で**Euler–Maruyama法による逆拡散サンプリング**を検証した結果を示します。左図は初期のデータ分布（平均0、分散0.04の正規分布）のヒストグラム、右図はそのデータを順SDE（VP-SDE相当のOrnstein–Uhlenbeck過程）で$t=5$まで拡散し、さらに学習済みスコア（真の分布のスコアを利用）を用いて逆SDEを数値解いた後のサンプル分布です。逆拡散による生成サンプルが初期のデータ分布と**ほぼ一致する**ことが確認できます。このように、適切にスコア関数が得られていれば、Euler–Maruyama法による数値解法で逆拡散SDEを解くことで元のデータ分布からのサンプルを生成できるのです。

以上、拡散モデルのSDE形式について、理論から実装までを概観しました。確率微分方程式という枠組みにより、拡散モデルはマルコフ連鎖による定式化から一歩進んで深い理論的基盤の上に統一されました。Ito過程とFokker–Planck方程式によってその動作を解析的に理解でき、また確率流ODEとの対応付けにより新たな可能性（正確な密度評価や高速サンプリング）が拓かれています。これら理論的洞察とディープラーニングによる関数近似（スコアネットワーク）を組み合わせることで、拡散モデルは画像生成をはじめ多くの分野で最先端の成果を上げており、今後もその発展が期待されています。

**参考文献**: SongらのScore-based Generative Models ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=,SDE%20solvers%20to%20generate%20samples)) ([[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456#:~:text=We%20show%20that%20this%20framework,In%20addition%2C%20we))、HoらのDDPM、ならびにFokker–Planck方程式の解説 ([FokkerPlanck_Equation](http://www.peterholderrieth.com/blog/2023/The-Fokker-Planck-Equation-and-Diffusion-Models/#:~:text=In%20fact%2C%20we%20can%20also,nabla%5Ccdot%20%5Bfp_t%2B%5Cfrac%7B1%7D%7B2%7D%5Cnabla%5ET%5Bgg%5ETp_t%5D%5D%5C%5C%20%5Cend%7Balign%2A%7D))など。拡散モデルの包括的な解説にはYang Songのブログ記事 ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=%5C%5B%5Cbegin,equation)) ([Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song](https://yang-song.net/blog/2021/score/#:~:text=%5C%5B%5Cbegin,aligned))やLilian Wengのブログ記事 ([What are Diffusion Models? | Lil'Log](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#:~:text=Diffusion%20models%20are%20inspired%20by,same%20as%20the%20original%20data))も参考になります。